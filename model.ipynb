{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "478ee13b-e9eb-420a-9b72-902eb33d8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4e7711ec-b2cf-4343-9766-d4baabf8d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocessing(d):  # 한국어 기사 본문 전처리 함수\n",
    "    d = d.lower()\n",
    "    d = re.sub(r'[a-z0-9\\-_.]{3,}@[a-z0-9\\-_.]{3,}(?:[.]?[a-z]{2})+', ' ', d)\n",
    "    d = re.sub(r'‘’ⓒ\\'\\\"“”…=□*◆:/_]', ' ', d)\n",
    "    d = re.sub(r'\\s+', ' ', d)\n",
    "    d = re.sub(r'^\\s|\\s$', '', d)\n",
    "    d = re.sub(r'[<*>_=\"/■□▷▶]', '', d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def fetch_article_data(article_url):  # 기사 본문, 기자 정보 수집 함수\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    resp = requests.get(article_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        return \"Failed to retrieve the article\"\n",
    "\n",
    "    article_dom = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "    # 특정 선택자를 사용하여 기사 본문 추출\n",
    "    content_tag = article_dom.select_one(\n",
    "        'article#dic_area.go_trans._article_content')\n",
    "\n",
    "    content = preprocessing(content_tag.get_text(\n",
    "        strip=True)) if content_tag else ''\n",
    "\n",
    "    # 기자 정보 추출\n",
    "    reporter_tag = article_dom.select_one('div.byline span') or \\\n",
    "        article_dom.select_one('p.byline') or \\\n",
    "        article_dom.select_one('span.byline')\n",
    "\n",
    "    reporter = reporter_tag.get_text(strip=True) if reporter_tag else ''\n",
    "\n",
    "    article_data = {\n",
    "        \"link\": article_url,  # 기사 링크\n",
    "        \"article\": content,  # 기사 본문\n",
    "        \"reporter\": reporter  # 기자\n",
    "    }\n",
    "\n",
    "    return article_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "95b8f8c1-df1e-455e-9d24-e57c7b15a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피어슨 상관계수 구하기\n",
    "def pearson_similarity(a, b):\n",
    "    return np.dot((a-np.mean(a)), (b-np.mean(b)))/((np.linalg.norm(a-np.mean(a)))*(np.linalg.norm(b-np.mean(b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7a97c3de-ce79-469d-91c8-d8128518b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pymysql\n",
    "\n",
    "# 피어슨 상관계수 구하기\n",
    "def pearson_similarity(a, b):\n",
    "    return np.dot((a-np.mean(a)), (b-np.mean(b)))/((np.linalg.norm(a-np.mean(a)))*(np.linalg.norm(b-np.mean(b))))\n",
    "\n",
    "# 현재 읽고 있는 기사와 유사한 기사 찾기\n",
    "\n",
    "\n",
    "def find_similar_news(target_summary, model):\n",
    "\n",
    "    # 저장된 임베딩 데이터 불러오기\n",
    "    #query = 'SELECT summary_embedding FROM db_summary_embeddings'\n",
    "    #db_summary_embeddings = get_embedding_dataset(query)\n",
    "\n",
    "    db_summary_embeddings=pd.read_json('dataset/summary_embedding.json')\n",
    "    db_summary_embeddings=db_summary_embeddings.sort_index()\n",
    "    db_embeddings=[]\n",
    "    for row in db_summary_embeddings['summary_embedding']:\n",
    "        db_embeddings.append(np.fromstring(row[1:-1], dtype=np.float32, sep=' '))\n",
    "\n",
    "    # 현재 읽고 있는 기사 요약문 임베딩\n",
    "    target_summary_embedding = model.encode(target_summary,\n",
    "                                            normalize_embeddings=True)\n",
    "\n",
    "    # 피어슨 상관계수 기반으로 계산\n",
    "    threshold = 0.55  # 최소 유사도 threshold\n",
    "    similar_list = []\n",
    "    for i in range(len(db_embeddings)):\n",
    "        similarity = pearson_similarity(\n",
    "            target_summary_embedding, db_embeddings[i])\n",
    "\n",
    "        if similarity > threshold:\n",
    "            # threshold 이상이면 유사한 기사 리스트에 추가\n",
    "            similar_list.append((similarity, i))\n",
    "\n",
    "    # 유사도 기준 내림차순 정렬\n",
    "    sorted_similar_list = sorted(\n",
    "        similar_list, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # 100개만 추려서 반환\n",
    "    if len(similar_list) > 100:\n",
    "        return [item[1] for item in sorted_similar_list[:100]]\n",
    "\n",
    "    # 100개 이하면 모두 반환\n",
    "    else:\n",
    "        return [item[1] for item in sorted_similar_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6a547077-87ac-48a1-928e-985d932ad734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: GPU available: False, used: False\n",
      "[Kss]: TPU available: False, using: 0 TPU cores\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jinmang2/kpfbert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# 요약 KPF-BERTSUMM\n",
    "# https://github.com/KPFBERT/kpfbertsum\n",
    "MAX_TOKEN_COUNT = 512\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "BERT_MODEL_NAME = 'jinmang2/kpfbert'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                              -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        if (step):\n",
    "            emb = emb + self.pe[:, step][:, None, :]\n",
    "\n",
    "        else:\n",
    "            emb = emb + self.pe[:, :emb.size(1)]\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "\n",
    "    def get_emb(self, emb):\n",
    "        return self.pe[:, :emb.size(1)]\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            heads, d_model, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, iter, query, inputs, mask):\n",
    "        if (iter != 0):\n",
    "            input_norm = self.layer_norm(inputs)\n",
    "        else:\n",
    "            input_norm = inputs\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
    "                                 mask=mask)\n",
    "        out = self.dropout(context) + inputs\n",
    "        return self.feed_forward(out)\n",
    "\n",
    "\n",
    "class ExtTransformerEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, d_ff=2048, heads=8, dropout=0.2, num_inter_layers=2):\n",
    "        super(ExtTransformerEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_inter_layers = num_inter_layers\n",
    "        self.pos_emb = PositionalEncoding(dropout, hidden_size)\n",
    "        self.transformer_inter = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(hidden_size, heads, d_ff, dropout)\n",
    "             for _ in range(num_inter_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.wo = nn.Linear(hidden_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, top_vecs, mask):\n",
    "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
    "\n",
    "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
    "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
    "        x = top_vecs * mask[:, :, None].float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for i in range(self.num_inter_layers):\n",
    "            x = self.transformer_inter[i](i, x, x, ~mask)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        sent_scores = self.sigmoid(self.wo(x))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "\n",
    "        return sent_scores\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): the size of input for the first-layer of the FFN.\n",
    "        d_ff (int): the hidden layer size of the second-layer\n",
    "            of the FNN.\n",
    "        dropout (float): dropout probability in :math:`[0, 1)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        inter = self.dropout_1(self.gelu(self.w_1(self.layer_norm(x))))\n",
    "        output = self.dropout_2(self.w_2(inter))\n",
    "        return output + x\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module from\n",
    "    \"Attention is All You Need\"\n",
    "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n",
    "\n",
    "    Similar to standard `dot` attention but uses\n",
    "    multiple attention distributions simulataneously\n",
    "    to select relevant items.\n",
    "\n",
    "    .. mermaid::\n",
    "\n",
    "       graph BT\n",
    "          A[key]\n",
    "          B[value]\n",
    "          C[query]\n",
    "          O[output]\n",
    "          subgraph Attn\n",
    "            D[Attn 1]\n",
    "            E[Attn 2]\n",
    "            F[Attn N]\n",
    "          end\n",
    "          A --> D\n",
    "          C --> D\n",
    "          A --> E\n",
    "          C --> E\n",
    "          A --> F\n",
    "          C --> F\n",
    "          D --> O\n",
    "          E --> O\n",
    "          F --> O\n",
    "          B --> O\n",
    "\n",
    "    Also includes several additional tricks.\n",
    "\n",
    "    Args:\n",
    "       head_count (int): number of parallel heads\n",
    "       model_dim (int): the dimension of keys/values/queries,\n",
    "           must be divisible by head_count\n",
    "       dropout (float): dropout parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
    "        assert model_dim % head_count == 0\n",
    "        self.dim_per_head = model_dim // head_count\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_count = head_count\n",
    "\n",
    "        self.linear_keys = nn.Linear(model_dim,\n",
    "                                     head_count * self.dim_per_head)\n",
    "        self.linear_values = nn.Linear(model_dim,\n",
    "                                       head_count * self.dim_per_head)\n",
    "        self.linear_query = nn.Linear(model_dim,\n",
    "                                      head_count * self.dim_per_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.use_final_linear = use_final_linear\n",
    "        if (self.use_final_linear):\n",
    "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, key, value, query, mask=None,\n",
    "                layer_cache=None, type=None, predefined_graph_1=None):\n",
    "        \"\"\"\n",
    "        Compute the context vector and the attention vectors.\n",
    "\n",
    "        Args:\n",
    "           key (`FloatTensor`): set of `key_len`\n",
    "                key vectors `[batch, key_len, dim]`\n",
    "           value (`FloatTensor`): set of `key_len`\n",
    "                value vectors `[batch, key_len, dim]`\n",
    "           query (`FloatTensor`): set of `query_len`\n",
    "                 query vectors  `[batch, query_len, dim]`\n",
    "           mask: binary mask indicating which keys have\n",
    "                 non-zero attention `[batch, query_len, key_len]`\n",
    "        Returns:\n",
    "           (`FloatTensor`, `FloatTensor`) :\n",
    "\n",
    "           * output context vectors `[batch, query_len, dim]`\n",
    "           * one of the attention vectors `[batch, query_len, key_len]`\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = key.size(0)\n",
    "        dim_per_head = self.dim_per_head\n",
    "        head_count = self.head_count\n",
    "        key_len = key.size(1)\n",
    "        query_len = query.size(1)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"  projection \"\"\"\n",
    "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
    "                .transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"  compute context \"\"\"\n",
    "            return x.transpose(1, 2).contiguous() \\\n",
    "                .view(batch_size, -1, head_count * dim_per_head)\n",
    "\n",
    "        # 1) Project key, value, and query.\n",
    "        if layer_cache is not None:\n",
    "            if type == \"self\":\n",
    "                query, key, value = self.linear_query(query), \\\n",
    "                    self.linear_keys(query), \\\n",
    "                    self.linear_values(query)\n",
    "\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "\n",
    "                if layer_cache is not None:\n",
    "                    device = key.device\n",
    "                    if layer_cache[\"self_keys\"] is not None:\n",
    "                        key = torch.cat(\n",
    "                            (layer_cache[\"self_keys\"].to(device), key),\n",
    "                            dim=2)\n",
    "                    if layer_cache[\"self_values\"] is not None:\n",
    "                        value = torch.cat(\n",
    "                            (layer_cache[\"self_values\"].to(device), value),\n",
    "                            dim=2)\n",
    "                    layer_cache[\"self_keys\"] = key\n",
    "                    layer_cache[\"self_values\"] = value\n",
    "            elif type == \"context\":\n",
    "                query = self.linear_query(query)\n",
    "                if layer_cache is not None:\n",
    "                    if layer_cache[\"memory_keys\"] is None:\n",
    "                        key, value = self.linear_keys(key), \\\n",
    "                            self.linear_values(value)\n",
    "                        key = shape(key)\n",
    "                        value = shape(value)\n",
    "                    else:\n",
    "                        key, value = layer_cache[\"memory_keys\"], \\\n",
    "                            layer_cache[\"memory_values\"]\n",
    "                    layer_cache[\"memory_keys\"] = key\n",
    "                    layer_cache[\"memory_values\"] = value\n",
    "                else:\n",
    "                    key, value = self.linear_keys(key), \\\n",
    "                        self.linear_values(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "        else:\n",
    "            key = self.linear_keys(key)\n",
    "            value = self.linear_values(value)\n",
    "            query = self.linear_query(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "\n",
    "        query = shape(query)\n",
    "\n",
    "        key_len = key.size(2)\n",
    "        query_len = query.size(2)\n",
    "\n",
    "        # 2) Calculate and scale scores.\n",
    "        query = query / math.sqrt(dim_per_head)\n",
    "        scores = torch.matmul(query, key.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)\n",
    "            # how can i fix it to use fp16...\n",
    "            scores = scores.masked_fill(mask, -1e18)\n",
    "\n",
    "        # 3) Apply attention dropout and compute context vectors.\n",
    "\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        if (not predefined_graph_1 is None):\n",
    "            attn_masked = attn[:, -1] * predefined_graph_1\n",
    "            attn_masked = attn_masked / \\\n",
    "                (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
    "\n",
    "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
    "\n",
    "        drop_attn = self.dropout(attn)\n",
    "        if (self.use_final_linear):\n",
    "            context = unshape(torch.matmul(drop_attn, value))\n",
    "            output = self.final_linear(context)\n",
    "            return output\n",
    "        else:\n",
    "            context = torch.matmul(drop_attn, value)\n",
    "            return context\n",
    "\n",
    "\n",
    "class Summarizer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.max_pos = 512\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            BERT_MODEL_NAME)  # , return_dict=True)\n",
    "        self.ext_layer = ExtTransformerEncoder()\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.loss = nn.BCELoss(reduction='none')\n",
    "\n",
    "        for p in self.ext_layer.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    # , input_ids, attention_mask, labels=None):\n",
    "    def forward(self, src, segs, clss, labels=None):\n",
    "\n",
    "        mask_src = ~(src == 0)  # 1 - (src == 0)\n",
    "        mask_cls = ~(clss == -1)  # 1 - (clss == -1)\n",
    "\n",
    "        top_vec = self.bert(src, token_type_ids=segs, attention_mask=mask_src)\n",
    "        top_vec = top_vec.last_hidden_state\n",
    "\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "\n",
    "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
    "\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss(sent_scores, labels)\n",
    "\n",
    "            loss = (loss * mask_cls.float()).sum() / len(labels)\n",
    "\n",
    "        return loss, sent_scores\n",
    "\n",
    "    def step(self, batch):\n",
    "\n",
    "        src = batch['src']\n",
    "        if len(batch['labels']) > 0:\n",
    "            labels = batch['labels']\n",
    "        else:\n",
    "            labels = None\n",
    "        segs = batch['segs']\n",
    "        clss = batch['clss']\n",
    "\n",
    "        loss, sent_scores = self(src, segs, clss, labels)\n",
    "\n",
    "        return loss, sent_scores, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def acc_loss(self, outputs):\n",
    "        total_loss = 0\n",
    "        hit_cnt = 0\n",
    "        for outp in outputs:\n",
    "            labels = outp['labels'].cpu()\n",
    "            predictions, idxs = outp['predictions'].cpu().sort()\n",
    "            loss = outp['loss'].cpu()\n",
    "            for label, idx in zip(labels, idxs):\n",
    "                for i in range(1, 3):\n",
    "                    if label[idx[-i-1]] == 1:\n",
    "                        hit_cnt += 1\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "        avg_loss = total_loss / len(outputs)\n",
    "        acc = hit_cnt / (3*len(outputs)*len(labels))\n",
    "\n",
    "        return acc, avg_loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        acc, avg_loss = self.acc_loss(outputs)\n",
    "\n",
    "        print('acc:', acc, 'avg_loss:', avg_loss)\n",
    "\n",
    "        self.log('avg_train_loss', avg_loss, prog_bar=True, logger=True)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        acc, avg_loss = self.acc_loss(outputs)\n",
    "\n",
    "        print('val_acc:', acc, 'avg_val_loss:', avg_loss)\n",
    "\n",
    "        self.log('avg_val_loss', avg_loss, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        acc, avg_loss = self.acc_loss(outputs)\n",
    "\n",
    "        print('test_acc:', acc, 'avg_test_loss:', avg_loss)\n",
    "\n",
    "        self.log('avg_test_loss', avg_loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "        steps_per_epoch = 10 // BATCH_SIZE\n",
    "        total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=steps_per_epoch,\n",
    "            num_training_steps=total_training_steps\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"avg_val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"kpfBERT_Summary\")\n",
    "early_stopping_callback = EarlyStopping(monitor='avg_val_loss', patience=3)\n",
    "trainer = pl.Trainer(\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=0,\n",
    "    #   precision=16, #소스 수정 또는 패키지 재설치 필요... 런타임 에러.\n",
    "    progress_bar_refresh_rate=30\n",
    ")\n",
    "\n",
    "\n",
    "trained_model = Summarizer.load_from_checkpoint(\n",
    "    'best-checkpoint.ckpt',\n",
    "    strict=False\n",
    ")\n",
    "trained_model.eval()\n",
    "trained_model.freeze()\n",
    "\n",
    "\n",
    "def data_process(text):\n",
    "    # 문장 분리 하고,\n",
    "    sents = kss.split_sentences(text)\n",
    "\n",
    "    # 데이터 가공하고,\n",
    "    tokenlist = []\n",
    "    for sent in sents:\n",
    "        tokenlist.append(tokenizer(\n",
    "            text=sent,\n",
    "            add_special_tokens=True))  # , # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "    src = []  # 토크나이징 된 전체 문단\n",
    "    labels = []  # 요약문에 해당하면 1, 아니면 0으로 문장수 만큼 생성\n",
    "    segs = []  # 각 토큰에 대해 홀수번째 문장이면 0, 짝수번째 문장이면 1을 매핑\n",
    "    clss = []  # [CLS]토큰의 포지션값을 지정\n",
    "\n",
    "    odd = 0\n",
    "\n",
    "    for tkns in tokenlist:\n",
    "\n",
    "        if odd > 1:\n",
    "            odd = 0\n",
    "        clss = clss + [len(src)]\n",
    "        src = src + tkns['input_ids']\n",
    "        segs = segs + [odd] * len(tkns['input_ids'])\n",
    "        odd += 1\n",
    "\n",
    "        # truncation\n",
    "        if len(src) == MAX_TOKEN_COUNT:\n",
    "            break\n",
    "        elif len(src) > MAX_TOKEN_COUNT:\n",
    "            src = src[:MAX_TOKEN_COUNT - 1] + [src[-1]]\n",
    "            segs = segs[:MAX_TOKEN_COUNT]\n",
    "            break\n",
    "\n",
    "    # padding\n",
    "    if len(src) < MAX_TOKEN_COUNT:\n",
    "        src = src + [0]*(MAX_TOKEN_COUNT - len(src))\n",
    "        segs = segs + [0]*(MAX_TOKEN_COUNT - len(segs))\n",
    "\n",
    "    if len(clss) < MAX_TOKEN_COUNT:\n",
    "        clss = clss + [-1]*(MAX_TOKEN_COUNT - len(clss))\n",
    "\n",
    "    return dict(\n",
    "        sents=sents,  # 정답 출력을 위해...\n",
    "        src=torch.tensor(src),\n",
    "        segs=torch.tensor(segs),\n",
    "        clss=torch.tensor(clss),\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize_test(text):\n",
    "    data = data_process(text.replace('\\n', ''))\n",
    "\n",
    "    # trained_model에 넣어 결과값 반환\n",
    "    _, rtn = trained_model(data['src'].unsqueeze(\n",
    "        0), data['segs'].unsqueeze(0), data['clss'].unsqueeze(0))\n",
    "    rtn = rtn.squeeze()\n",
    "\n",
    "    # 예측 결과값을 받기 위한 프로세스\n",
    "    rtn_sort, idx = rtn.sort(descending=True)\n",
    "\n",
    "    rtn_sort = rtn_sort.tolist()\n",
    "    idx = idx.tolist()\n",
    "\n",
    "    end_idx = rtn_sort.index(0)\n",
    "\n",
    "    rtn_sort = rtn_sort[:end_idx]\n",
    "    idx = idx[:end_idx]\n",
    "\n",
    "    if len(idx) > 3:\n",
    "        rslt = idx[:3]\n",
    "    else:\n",
    "        rslt = idx\n",
    "\n",
    "    summ = []\n",
    "    # print(' *** 입력한 문단의 요약문은 ...')\n",
    "    for i, r in enumerate(rslt):\n",
    "        summ.append(data['sents'][r])\n",
    "        # print('[', i+1, ']', summ[i])\n",
    "\n",
    "    return summ\n",
    "\n",
    "\n",
    "def summarize_article(target_article):\n",
    "    target_summary = summarize_test(target_article)\n",
    "    return target_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ffd5e833-2164-40fa-bffc-830b53fb2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "# 3줄씩 문장을 잘라 단락 생성\n",
    "\n",
    "\n",
    "def split_into_paragraphs(article, sentences_per_paragraph=3):\n",
    "    sentences = kss.split_sentences(article)\n",
    "    paragraphs = []\n",
    "    paragraph = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 20:\n",
    "            # 보통 한 줄에 20자 정도 넘어가야 유의미한 정보가 포함된 문장\n",
    "            paragraph.append(sentence)\n",
    "        if len(paragraph) == sentences_per_paragraph:  # 3줄 이상이면\n",
    "            paragraphs.append(\" \".join(paragraph))  # 3줄을 하나로 합치기\n",
    "            paragraph = []\n",
    "\n",
    "        # 남아있는 문장들 중 20자가 넘어가면 단락으로 추가\n",
    "    if paragraph and len(paragraph) > 20:\n",
    "        paragraphs.append(\" \".join(paragraph))\n",
    "\n",
    "    return paragraphs  # 단락 데이터 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e0e0e2f8-120d-4c49-9ae1-3543e5441e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# 클러스터\n",
    "def clustering(target_article, similar_news):\n",
    "    db_paragraph_data = pd.read_json('dataset/paragraph_data.json')\n",
    "    db_paragraph_embeddings1 = pd.read_json('dataset/paragraph_embedding.json') # DB에서 단락임베딩 데이터 가져오기\n",
    "    db_paragraph_embeddings=[]\n",
    "    for row in db_paragraph_embeddings1['paragraph_embedding']:\n",
    "        db_paragraph_embeddings.append(np.fromstring(row[1:-1], dtype=np.float32, sep=' '))\n",
    "\n",
    "    db_paragraph_embeddings = np.array(db_embeddings)\n",
    "    \n",
    "    db_paragraph_embeddings = db_paragraph_embeddings[db_paragraph_data['index'].isin(\n",
    "        similar_news)]  # 유사한 기사 데이터만 가져오기\n",
    "    db_paragraph_data = db_paragraph_data[db_paragraph_data['index'].isin(\n",
    "        similar_news)]  # 유사한 기사 데이터만 가져오기\n",
    "\n",
    "    # 현재 읽고 있는 기사의 단락 데이터\n",
    "    target_paragraphs = split_into_paragraphs(target_article)\n",
    "    target_paragraph_data = []\n",
    "    for data in target_paragraphs:\n",
    "        target_paragraph_data.append([-1]+[data])\n",
    "\n",
    "    target_paragraph_data = pd.DataFrame(\n",
    "        data=target_paragraph_data,\n",
    "        columns=['index', 'paragraph'])\n",
    "\n",
    "    model = SentenceTransformer('bongsoo/kpf-sbert-128d-v1')\n",
    "\n",
    "    target_embeddings = model.encode(\n",
    "        target_paragraph_data['paragraph'].tolist())  # 현재 읽고 있는 기사 단락 임베딩\n",
    "\n",
    "    # 현재 읽고 있는 기사 데이터와 유사한 기사 데이터를 합쳐서 훈련 데이터로 들어감\n",
    "    train_paragraph_embeddings = np.vstack(\n",
    "        (target_embeddings, db_paragraph_embeddings))\n",
    "    train_paragraph_data = pd.concat(\n",
    "        [target_paragraph_data, db_paragraph_data], axis=0)\n",
    "\n",
    "    # BERTopic을 이용한 클러스터링\n",
    "    model = BERTopic(embedding_model='bongsoo/kpf-sbert-128d-v1',\n",
    "                     min_topic_size=5)\n",
    "\n",
    "    topics, probs = model.fit_transform(\n",
    "        documents=train_paragraph_data['paragraph'], embeddings=train_paragraph_embeddings)  # 클러스터링 만들기\n",
    "    train_paragraph_data['topic'] = topics  # 토픽 저장\n",
    "\n",
    "    # 현재 읽고 있는 기사의 토픽 모델링\n",
    "    target_paragraph_data = pd.merge(target_paragraph_data, train_paragraph_data[[\n",
    "                                     'paragraph', 'topic']], on='paragraph', how='inner')\n",
    "    # 토픽이 -1, 0은 제외\n",
    "    target_paragraph_data = target_paragraph_data[target_paragraph_data['topic'] > 0]\n",
    "\n",
    "    if len(target_paragraph_data) == 0:  # 만약 현재 읽고 있는 기사의 토픽이 없으면\n",
    "        print('No Topic')\n",
    "        return similar_news  # 아무 기사3개 랜덤으로\n",
    "\n",
    "    # 유사한 기사들의 토픽 모델링 결과 저장\n",
    "    db_paragraph_data = pd.merge(db_paragraph_data, train_paragraph_data[[\n",
    "        'paragraph', 'topic']], on='paragraph', how='inner')\n",
    "    # 토픽 -1, 0 제외\n",
    "    db_paragraph_data = db_paragraph_data[db_paragraph_data['topic'] > 0]\n",
    "\n",
    "    # 토픽 간 거리 구하기\n",
    "    topic_embeddings = model.topic_embeddings_\n",
    "    topic_embeddings = topic_embeddings[1:]\n",
    "\n",
    "    target_topic = target_paragraph_data['topic'].value_counts().idxmax()\n",
    "    target_topic_embedding = topic_embeddings[target_topic]\n",
    "\n",
    "    # 현재 토픽 개수 0~n\n",
    "    num_topics = len(model.get_topic_freq()) - 1\n",
    "\n",
    "    # faiss를 이용해서 토픽 간 코사인 유사도 계산\n",
    "    index = faiss.IndexFlatIP(128)\n",
    "    faiss.normalize_L2(topic_embeddings)\n",
    "    index.add(topic_embeddings)\n",
    "    distances, indices = index.search(np.expand_dims(\n",
    "        target_topic_embedding, axis=0), num_topics)\n",
    "\n",
    "    # 가장 유사도가 낮은 토픽 순으로 단락 정렬\n",
    "    indices = indices[0][::-1]\n",
    "    indices = np.delete(indices, np.where(indices == 0)[0][0])\n",
    "    db_paragraph_data['topic'] = pd.Categorical(\n",
    "        db_paragraph_data['topic'], categories=indices, ordered=True)\n",
    "    db_paragraph_data = db_paragraph_data.sort_values('topic')\n",
    "\n",
    "    # 토픽이 3개 이상이면\n",
    "    if num_topics - 2 > 3:\n",
    "        index_counts = db_paragraph_data.groupby(\n",
    "            'topic')['index'].value_counts().rename('count').reset_index()\n",
    "        most_common_index_per_topic = index_counts.loc[index_counts.groupby('topic')[\n",
    "            'count'].idxmax()]\n",
    "        most_common_index_per_topic = most_common_index_per_topic.drop_duplicates(\n",
    "            subset='index')  # 중복 제거\n",
    "\n",
    "        return most_common_index_per_topic['index'].tolist()\n",
    "\n",
    "    else:  # 토픽이 3개 이하이면 나온 것 모두 반환\n",
    "        db_paragraph_data = db_paragraph_data.drop_duplicates(\n",
    "            subset='index')  # 중복 제거\n",
    "        return db_paragraph_data['index'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "43cd709a-6777-415b-84ef-2eca715e8456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5732</th>\n",
       "      <td>5732</td>\n",
       "      <td>[기획] `전국민 25만원` 제동 건 기재부</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/029/000...</td>\n",
       "      <td>민주당, 15조 추경 편성 제안尹·李 회담 최대 의제로 꼽혀기재부, '부정적' 입장...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>5664</td>\n",
       "      <td>대통령실 “민생지원금은 포퓰리즘” 인식… 협치위해 ‘선별지원’ 가능성</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/021/000...</td>\n",
       "      <td>대상·금액 조정해 합의할수도민주 “부자감세 줄여 재원마련”이재명 더불어민주당 대표가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>5594</td>\n",
       "      <td>입법권으로 민생지원금 주자?… 삼권분립 훼손하는 巨野 [심층기획-'처분적 법률' 위...</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/022/000...</td>\n",
       "      <td>“법률 개념 자체가 위헌적 뉘앙스”행정·재판 없이 국민에 직접 자동집행력5·18민주...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title  \\\n",
       "5732   5732                           [기획] `전국민 25만원` 제동 건 기재부   \n",
       "5664   5664             대통령실 “민생지원금은 포퓰리즘” 인식… 협치위해 ‘선별지원’ 가능성   \n",
       "5594   5594  입법권으로 민생지원금 주자?… 삼권분립 훼손하는 巨野 [심층기획-'처분적 법률' 위...   \n",
       "\n",
       "                                                   link  \\\n",
       "5732  https://n.news.naver.com/mnews/article/029/000...   \n",
       "5664  https://n.news.naver.com/mnews/article/021/000...   \n",
       "5594  https://n.news.naver.com/mnews/article/022/000...   \n",
       "\n",
       "                                                article  \n",
       "5732  민주당, 15조 추경 편성 제안尹·李 회담 최대 의제로 꼽혀기재부, '부정적' 입장...  \n",
       "5664  대상·금액 조정해 합의할수도민주 “부자감세 줄여 재원마련”이재명 더불어민주당 대표가...  \n",
       "5594  “법률 개념 자체가 위헌적 뉘앙스”행정·재판 없이 국민에 직접 자동집행력5·18민주...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('inference_dataset/url.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    \n",
    "url=json_data['url']\n",
    "target_data = fetch_article_data(url)  # 현재 읽고 있는 뉴스 크롤링\n",
    "target_article = target_data['article']\n",
    "target_summary1 = summarize_article(target_article)\n",
    "target_summary = \" \".join(target_summary1)\n",
    "model = SentenceTransformer('bongsoo/kpf-sbert-128d-v1')  # 임베딩 모델\n",
    "similar_news_index = find_similar_news(\n",
    "        target_summary, model)  # 유사한 기사 찾기 (인덱스 반환)\n",
    "\n",
    "\n",
    "various_news_index = clustering(target_article, similar_news_index)  # 클러스터링\n",
    "various_news = pd.read_json('dataset/news.json')\n",
    "if url in various_news['link']:\n",
    "        same_news_index=various_news[various_news['link']==url].index\n",
    "        various_news_index.remove(same_news_index)\n",
    "\n",
    "\n",
    "various_news = various_news.loc[various_news_index][:3]\n",
    "\n",
    "various_news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
